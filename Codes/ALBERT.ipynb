{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxAkXurO4ed2"
   },
   "source": [
    "Sentiment Analysis for Cyberbullying Detection on Social Media Platforms using ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>soon sad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons of why couldnt they put them on the relea...</td>\n",
       "      <td>sons of</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on denver husband los...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>ive wondered about rake to the client has made...</td>\n",
       "      <td>dont force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>yay good for both of you enjoy the break you p...</td>\n",
       "      <td>yay good for both of you</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>but it was worth it</td>\n",
       "      <td>but it was worth it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>all this flirting going on the atg smiles yay ...</td>\n",
       "      <td>all this flirting going on the at smiles yay hugs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                  id have responded if i were going   \n",
       "1      549e992a42         sooo sad i will miss you here in san diego   \n",
       "2      088c60f138                             my boss is bullying me   \n",
       "3      9642c003ef                      what interview leave me alone   \n",
       "4      358bd9e861  sons of why couldnt they put them on the relea...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0  wish we could come see u on denver husband los...   \n",
       "27477  4f4c4fc327  ive wondered about rake to the client has made...   \n",
       "27478  f67aae2310  yay good for both of you enjoy the break you p...   \n",
       "27479  ed167662a5                                but it was worth it   \n",
       "27480  6f7127d9d7  all this flirting going on the atg smiles yay ...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                      id have responded if i were going   neutral  \n",
       "1                                               soon sad  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                                sons of  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                         dont force  negative  \n",
       "27478                           yay good for both of you  positive  \n",
       "27479                                but it was worth it  positive  \n",
       "27480  all this flirting going on the at smiles yay hugs   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../Datasets/preprocessed_train.csv\")\n",
    "dataset\n",
    "\n",
    "#dataset = pd.read_csv(\"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv\")\n",
    "#dataset = dataset.sample(10_000)\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "o8f8_Z0kz4iO"
   },
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 1327,
     "status": "ok",
     "timestamp": 1609351552294,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "ZUrUmrmjz4k1",
    "outputId": "05f304a9-8b0d-4a6d-a1d9-7e5f1954d3c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>soon sad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons of why couldnt they put them on the relea...</td>\n",
       "      <td>sons of</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on denver husband los...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>ive wondered about rake to the client has made...</td>\n",
       "      <td>dont force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>yay good for both of you enjoy the break you p...</td>\n",
       "      <td>yay good for both of you</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>but it was worth it</td>\n",
       "      <td>but it was worth it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>all this flirting going on the atg smiles yay ...</td>\n",
       "      <td>all this flirting going on the at smiles yay hugs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27383 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                  id have responded if i were going   \n",
       "1      549e992a42         sooo sad i will miss you here in san diego   \n",
       "2      088c60f138                             my boss is bullying me   \n",
       "3      9642c003ef                      what interview leave me alone   \n",
       "4      358bd9e861  sons of why couldnt they put them on the relea...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0  wish we could come see u on denver husband los...   \n",
       "27477  4f4c4fc327  ive wondered about rake to the client has made...   \n",
       "27478  f67aae2310  yay good for both of you enjoy the break you p...   \n",
       "27479  ed167662a5                                but it was worth it   \n",
       "27480  6f7127d9d7  all this flirting going on the atg smiles yay ...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                      id have responded if i were going   neutral  \n",
       "1                                               soon sad  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                                sons of  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                         dont force  negative  \n",
       "27478                           yay good for both of you  positive  \n",
       "27479                                but it was worth it  positive  \n",
       "27480  all this flirting going on the at smiles yay hugs   neutral  \n",
       "\n",
       "[27383 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "happy mothers day                                                                                                    36\n",
       "thanks                                                                                                               13\n",
       "good morning                                                                                                         12\n",
       "thank you                                                                                                            10\n",
       "goodnight                                                                                                             7\n",
       "                                                                                                                     ..\n",
       "hanging out with sam billy and veronica not going to school tomorrow to take sam to body shops dang car accidents     1\n",
       "eeeeeeeeeee it came                                                                                                   1\n",
       "great reason ill be there trying to find an excuse to go to la for show                                               1\n",
       "guess it was too much to hope for me to have a nice relaxing mothers day happy mothers day mommies                    1\n",
       "all this flirting going on the atg smiles yay hugs                                                                    1\n",
       "Name: count, Length: 27115, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for i in dataset['text']:\n",
    "  length.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3205., 4570., 4483., 3866., 3263., 2778., 2508., 1884.,  727.,\n",
       "          99.]),\n",
       " array([ 1. ,  4.1,  7.2, 10.3, 13.4, 16.5, 19.6, 22.7, 25.8, 28.9, 32. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Integrate sentiment score (you can use TextBlob or Vader here)\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#def get_sentiment(text):\n",
    "    #return TextBlob(text).sentiment.polarity\n",
    "\n",
    "#dataset['sentiment'] = dataset['selected_text'].apply(get_sentiment)\n",
    "#dataset['text'] = dataset['sentiment'].astype(str) + ' ' + dataset['selected_text']  # prepend sentiment to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1609351584653,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "dpwDnTHA48Vc",
    "outputId": "2c4a04fa-dc6e-41bb-9561-600cdec6ff08"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.16</td>\n",
       "      <td>754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.20</td>\n",
       "      <td>821.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment   label\n",
       "0        0.00   459.0\n",
       "1       -0.50    45.0\n",
       "2        0.00   459.0\n",
       "3        0.00   459.0\n",
       "4        0.00   459.0\n",
       "5        1.00  1206.0\n",
       "6        0.30   963.0\n",
       "7        0.16   754.0\n",
       "8        0.00   459.0\n",
       "9        0.10   659.0\n",
       "10       0.20   821.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "\r\n",
    "ord_enc = OrdinalEncoder()\r\n",
    "dataset[\"label\"] = ord_enc.fit_transform(dataset[[\"sentiment\"]])\r\n",
    "dataset[[\"sentiment\", \"label\"]].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1609351588399,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "aki1pDzn50hg",
    "outputId": "0341f255-8c00-4abf-e14a-2cca6c47a15c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>0.0 id have responded if i were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>0.0</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>-0.5 soon sad</td>\n",
       "      <td>soon sad</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>0.0 bullying me</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>0.0</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>0.0 leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>0.0</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>0.0 sons of</td>\n",
       "      <td>sons of</td>\n",
       "      <td>0.0</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                   text  \\\n",
       "0  cb774db0d1  0.0 id have responded if i were going   \n",
       "1  549e992a42                          -0.5 soon sad   \n",
       "2  088c60f138                        0.0 bullying me   \n",
       "3  9642c003ef                     0.0 leave me alone   \n",
       "4  358bd9e861                            0.0 sons of   \n",
       "\n",
       "                       selected_text  sentiment  label  \n",
       "0  id have responded if i were going        0.0  459.0  \n",
       "1                           soon sad       -0.5   45.0  \n",
       "2                        bullying me        0.0  459.0  \n",
       "3                     leave me alone        0.0  459.0  \n",
       "4                            sons of        0.0  459.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1609351590803,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "0TyOfPIu6EtO",
    "outputId": "e29cdb51-b9c5-49f8-9aaa-bdacfc6858bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27383, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this for case 2\n",
    "dataset = dataset[['selected_text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1609351593576,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "R_GWhFJ5_W4E",
    "outputId": "b52c05a0-a01d-48b7-b686-4677ecf6a98f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soon sad</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sons of</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       selected_text  label\n",
       "0  id have responded if i were going  459.0\n",
       "1                           soon sad   45.0\n",
       "2                        bullying me  459.0\n",
       "3                     leave me alone  459.0\n",
       "4                            sons of  459.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1609352321768,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "0CPo45-w54ez",
    "outputId": "2950eeae-4f0b-49f5-f858-2ff548f30a8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19236, 2), (8147, 2))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train and val split\r\n",
    "\r\n",
    "train_df = dataset.iloc[:19236]\r\n",
    "vall_df = dataset.iloc[19236:]\r\n",
    "train_df.shape, vall_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1609352336462,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "hDlLFDsM38qS",
    "outputId": "50042d0e-8947-4fa6-9597-9da6d2fb2ce8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4122, 2), (4025, 2))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val test split\r\n",
    "\r\n",
    "val_df = vall_df.iloc[:4122]\r\n",
    "test_df = vall_df.iloc[4122:]\r\n",
    "val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "nBYKVQtm7qQx"
   },
   "outputs": [],
   "source": [
    "save_dir = \"../data/\"\r\n",
    "train_df.to_csv(save_dir + \"train.csv\", index=False)\r\n",
    "val_df.to_csv(save_dir + \"dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Im_CMsfg7XML"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(save_dir + \"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "sGZz9CDp6Nmq"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\r\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\r\n",
    "        \"\"\"Constructs a InputExample.\r\n",
    "        Args:\r\n",
    "            guid: Unique id for the example.\r\n",
    "            text_a: string. The untokenized text of the first sequence. For single\r\n",
    "            sequence tasks, only this sequence must be specified.\r\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\r\n",
    "            Only must be specified for sequence pair tasks.\r\n",
    "            label: (Optional) string. The label of the example. This should be\r\n",
    "            specified for train and dev examples, but not for test examples.\r\n",
    "        \"\"\"\r\n",
    "        self.guid = guid\r\n",
    "        self.text_a = text_a\r\n",
    "        self.text_b = text_b\r\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jQvqkYnY6ZkZ"
   },
   "outputs": [],
   "source": [
    "class InputFeatures(object):\r\n",
    "    \"\"\"A single set of features of data.\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\r\n",
    "        self.input_ids = input_ids\r\n",
    "        self.input_mask = input_mask\r\n",
    "        self.segment_ids = segment_ids\r\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "KycuyRUt6d4w"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "class DataProcessor(object):\r\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\r\n",
    "\r\n",
    "    def get_train_examples(self, data_dir):\r\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "\r\n",
    "    def get_dev_examples(self, data_dir):\r\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "\r\n",
    "    def get_labels(self):\r\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "\r\n",
    "    @classmethod\r\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\r\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\r\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\r\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\r\n",
    "            lines = []\r\n",
    "            for line in reader:\r\n",
    "                if sys.version_info[0] == 2:\r\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\r\n",
    "                lines.append(line)\r\n",
    "            return lines\r\n",
    "          \r\n",
    "class TweetProcessor(DataProcessor):\r\n",
    "    \"\"\"Processor for the Amazon Reviews data set.\"\"\"\r\n",
    "\r\n",
    "    def get_train_examples(self, data_dir):\r\n",
    "        \"\"\"See base class.\"\"\"\r\n",
    "        return self._create_examples(\r\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.csv\")), \"train\")\r\n",
    "\r\n",
    "    def get_dev_examples(self, data_dir):\r\n",
    "        \"\"\"See base class.\"\"\"\r\n",
    "        return self._create_examples(\r\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.csv\")), \"dev\")\r\n",
    "\r\n",
    "    def get_test_examples(self, data_dir):\r\n",
    "        \"\"\"See base class.\"\"\"\r\n",
    "        return self._create_examples(\r\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.csv\")), \"test\")\r\n",
    "\r\n",
    "    def get_labels(self):\r\n",
    "        \"\"\"See base class.\"\"\"\r\n",
    "        return [0, 1, 2]\r\n",
    "\r\n",
    "    #Hack to be compatible with the existing code in transformers library\r\n",
    "    def _read_tsv(self, file_path):\r\n",
    "        return pd.read_csv(file_path).values.tolist()\r\n",
    "\r\n",
    "    def _create_examples(self, lines, set_type):\r\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\r\n",
    "        examples = []\r\n",
    "        for (i, line) in enumerate(lines):\r\n",
    "            if i == 0:\r\n",
    "               continue\r\n",
    "            guid = \"%s-%s\" % (set_type, i)\r\n",
    "            text_a = str(line[0])\r\n",
    "           # text_b = None\r\n",
    "            label = line[1]\r\n",
    "            examples.append(\r\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\r\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "6uM9OJal6iF-"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\r\n",
    "    random.seed(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    torch.manual_seed(seed)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"f1 Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall\n",
    "    }\n",
    "\n",
    "def precision_weighted(preds, labels):\n",
    "    return precision_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "\n",
    "def recall_weighted(preds, labels):\n",
    "    return recall_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "\n",
    "def f1_weighted(preds, labels):\n",
    "    return f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels), \"Predictions and labels must have the same length.\"\n",
    "    \n",
    "    if task_name == \"tweet\":\n",
    "        acc = simple_accuracy(preds, labels)  # Define acc before returning\n",
    "        return {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": precision_weighted(preds, labels),\n",
    "            \"Recall\": recall_weighted(preds, labels),\n",
    "            \"f1 Score\": f1_weighted(preds, labels)\n",
    "        }\n",
    "    else:\n",
    "        raise KeyError(f\"Unknown task name: {task_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Wk2MjBbu6npD"
   },
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\r\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\r\n",
    "\r\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\r\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\r\n",
    "    # of tokens from each, since if one sequence is very short then each token\r\n",
    "    # that's truncated likely contains more information than a longer sequence.\r\n",
    "    while True:\r\n",
    "        total_length = len(tokens_a) + len(tokens_b)\r\n",
    "        if total_length <= max_length:\r\n",
    "            break\r\n",
    "        if len(tokens_a) > len(tokens_b):\r\n",
    "            tokens_a.pop()\r\n",
    "        else:\r\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "tRP-i1VR6qoL"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length,\r\n",
    "                                 tokenizer, output_mode,\r\n",
    "                                 cls_token_at_end=False, pad_on_left=False,\r\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\r\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\r\n",
    "                                 cls_token_segment_id=1, pad_token_segment_id=0,\r\n",
    "                                 mask_padding_with_zero=True):\r\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\r\n",
    "        `cls_token_at_end` define the location of the CLS token:\r\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\r\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\r\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\r\n",
    "\r\n",
    "    features = []\r\n",
    "    for (ex_index, example) in enumerate(examples):\r\n",
    "\r\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\r\n",
    "\r\n",
    "        tokens_b = None\r\n",
    "        if example.text_b:\r\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\r\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\r\n",
    "            # length is less than the specified length.\r\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\r\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\r\n",
    "        else:\r\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\r\n",
    "            if len(tokens_a) > max_seq_length - 2:\r\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\r\n",
    "\r\n",
    "        # The convention in BERT is:\r\n",
    "        # (a) For sequence pairs:\r\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\r\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\r\n",
    "        # (b) For single sequences:\r\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\r\n",
    "        #  type_ids:   0   0   0   0  0     0   0\r\n",
    "        #\r\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\r\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\r\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\r\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\r\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\r\n",
    "        # it easier for the model to learn the concept of sequences.\r\n",
    "        #\r\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\r\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\r\n",
    "        # the entire model is fine-tuned.\r\n",
    "        tokens = tokens_a + [sep_token]\r\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\r\n",
    "\r\n",
    "        if tokens_b:\r\n",
    "            tokens += tokens_b + [sep_token]\r\n",
    "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\r\n",
    "\r\n",
    "        if cls_token_at_end:\r\n",
    "            tokens = tokens + [cls_token]\r\n",
    "            segment_ids = segment_ids + [cls_token_segment_id]\r\n",
    "        else:\r\n",
    "            tokens = [cls_token] + tokens\r\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\r\n",
    "\r\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "\r\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
    "        # tokens are attended to.\r\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\r\n",
    "\r\n",
    "        # Zero-pad up to the sequence length.\r\n",
    "        padding_length = max_seq_length - len(input_ids)\r\n",
    "        if pad_on_left:\r\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\r\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\r\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\r\n",
    "        else:\r\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\r\n",
    "            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\r\n",
    "            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\r\n",
    "\r\n",
    "        assert len(input_ids) == max_seq_length\r\n",
    "        assert len(input_mask) == max_seq_length\r\n",
    "        assert len(segment_ids) == max_seq_length\r\n",
    "\r\n",
    "        if output_mode == \"classification\":\r\n",
    "            label_id = label_map[example.label]\r\n",
    "        elif output_mode == \"regression\":\r\n",
    "            label_id = float(example.label)\r\n",
    "        else:\r\n",
    "            raise KeyError(output_mode)\r\n",
    "\r\n",
    "        features.append(\r\n",
    "                InputFeatures(input_ids=input_ids,\r\n",
    "                              input_mask=input_mask,\r\n",
    "                              segment_ids=segment_ids,\r\n",
    "                              label_id=label_id))\r\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "D_bnAV7b60ZK"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "processor = TweetProcessor()\r\n",
    "label_list = processor.get_labels()\r\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer\n",
    "config = AlbertConfig.from_pretrained('albert-base-v2', num_labels=num_labels)\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "9RFaKP0l8kRK"
   },
   "outputs": [],
   "source": [
    " def load_and_cache_examples(tokenizer, dataset='train'):  \r\n",
    "  if dataset == \"train\":\r\n",
    "      examples = processor.get_train_examples(data_dir)\r\n",
    "  elif dataset == \"dev\":\r\n",
    "      examples = processor.get_dev_examples(data_dir)\r\n",
    "  else:\r\n",
    "      examples = processor.get_test_examples(data_dir)\r\n",
    "  \r\n",
    "  features = convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_mode,\r\n",
    "            cls_token_at_end=True,            # xlnet has a cls token at the end\r\n",
    "            cls_token=tokenizer.cls_token,\r\n",
    "            sep_token=tokenizer.sep_token,\r\n",
    "            cls_token_segment_id=2,\r\n",
    "            pad_on_left=True,               # pad on the left for xlnet\r\n",
    "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\r\n",
    "            pad_token_segment_id=4)\r\n",
    "  # Convert to Tensors and build dataset\r\n",
    "  all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\r\n",
    "  all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\r\n",
    "  all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\r\n",
    "  if output_mode == \"classification\":\r\n",
    "      all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\r\n",
    "  elif output_mode == \"regression\":\r\n",
    "      all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\r\n",
    "\r\n",
    "  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\r\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "output_mode = 'classification'\n",
    "max_seq_length = 60\n",
    "batch_size = 8\n",
    "max_grad_norm = 1.0\n",
    "gradient_accumulation_steps=2\n",
    "num_train_epochs=1\n",
    "weight_decay=0.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "y4shGAUC-RfA"
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\r\n",
    "adam_epsilon = 1e-8\r\n",
    "num_warmup_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "HGeQmWDw-bYN"
   },
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer):\r\n",
    "  \"\"\" Train the model \"\"\"\r\n",
    "  train_sampler = RandomSampler(train_dataset)\r\n",
    "  train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\r\n",
    "  num_training_steps = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\r\n",
    "  # Prepare optimizer and schedule (linear warmup and decay)\r\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\r\n",
    "  optimizer_grouped_parameters = [\r\n",
    "      {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\r\n",
    "      {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\r\n",
    "      ]\r\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\r\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= num_warmup_steps,num_training_steps = num_training_steps)\r\n",
    "  \r\n",
    "  global_step = 0\r\n",
    "  tr_loss, logging_loss = 0.0, 0.0\r\n",
    "  model.zero_grad()\r\n",
    "  train_iterator = tqdm_notebook(range(int(num_train_epochs)), desc=\"Epoch\")\r\n",
    "  set_seed(42)\r\n",
    "  for _ in train_iterator:\r\n",
    "    epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\r\n",
    "    for step, batch in enumerate(epoch_iterator):\r\n",
    "      model.train()\r\n",
    "      batch = tuple(t.to(device) for t in batch)\r\n",
    "      inputs = {'input_ids':      batch[0],\r\n",
    "                'attention_mask': batch[1],\r\n",
    "                'token_type_ids': None,       # XLM and RoBERTa don't use segment_ids\r\n",
    "                'labels':         batch[3]}\r\n",
    "      outputs = model(**inputs)\r\n",
    "      loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\r\n",
    "      if gradient_accumulation_steps > 1:\r\n",
    "        loss = loss / gradient_accumulation_steps\r\n",
    "      loss.backward()\r\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\r\n",
    "      tr_loss += loss.item()\r\n",
    "      if (step + 1) % gradient_accumulation_steps == 0:\r\n",
    "          scheduler.step()  # Update learning rate schedule\r\n",
    "          optimizer.step()\r\n",
    "          model.zero_grad()\r\n",
    "          global_step += 1\r\n",
    "          \r\n",
    "  return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "6dWLmYAJ-euq"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, prefix=\"\"):\r\n",
    "  results = {}\r\n",
    "  eval_dataset = load_and_cache_examples(tokenizer, dataset='dev')\r\n",
    "  eval_batch_size = 8\r\n",
    "  eval_sampler = SequentialSampler(eval_dataset)\r\n",
    "  eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\r\n",
    "  eval_loss = 0.0\r\n",
    "  nb_eval_steps = 0\r\n",
    "  preds = None\r\n",
    "  out_label_ids = None\r\n",
    "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\r\n",
    "    model.eval()\r\n",
    "    batch = tuple(t.to(device) for t in batch)\r\n",
    "    with torch.no_grad():\r\n",
    "      inputs = {'input_ids':      batch[0],\r\n",
    "                'attention_mask': batch[1],\r\n",
    "                'token_type_ids': None,             # XLM and RoBERTa don't use segment_ids\r\n",
    "                'labels':         batch[3]}\r\n",
    "      outputs = model(**inputs)\r\n",
    "      tmp_eval_loss, logits = outputs[:2]\r\n",
    "      eval_loss += tmp_eval_loss.mean().item()\r\n",
    "    \r\n",
    "    nb_eval_steps += 1\r\n",
    "    if preds is None:\r\n",
    "        preds = logits.detach().cpu().numpy()\r\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\r\n",
    "    else:\r\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\r\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\r\n",
    "  eval_loss = eval_loss / nb_eval_steps\r\n",
    "  if output_mode == \"classification\":\r\n",
    "      preds = np.argmax(preds, axis=1)\r\n",
    "  elif output_mode == \"regression\":\r\n",
    "      preds = np.squeeze(preds)\r\n",
    "  result = compute_metrics(\"tweet\", preds, out_label_ids)\r\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "45.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dir\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m load_and_cache_examples(tokenizer, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m global_step, tr_loss \u001b[38;5;241m=\u001b[39m train(train_dataset, model, tokenizer)\n",
      "Cell \u001b[1;32mIn[62], line 9\u001b[0m, in \u001b[0;36mload_and_cache_examples\u001b[1;34m(tokenizer, dataset)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     examples \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mget_test_examples(data_dir)\n\u001b[1;32m----> 9\u001b[0m features \u001b[38;5;241m=\u001b[39m convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_mode,\n\u001b[0;32m     10\u001b[0m           cls_token_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,            \u001b[38;5;66;03m# xlnet has a cls token at the end\u001b[39;00m\n\u001b[0;32m     11\u001b[0m           cls_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcls_token,\n\u001b[0;32m     12\u001b[0m           sep_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msep_token,\n\u001b[0;32m     13\u001b[0m           cls_token_segment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     14\u001b[0m           pad_on_left\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,               \u001b[38;5;66;03m# pad on the left for xlnet\u001b[39;00m\n\u001b[0;32m     15\u001b[0m           pad_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([tokenizer\u001b[38;5;241m.\u001b[39mpad_token])[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     16\u001b[0m           pad_token_segment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert to Tensors and build dataset\u001b[39;00m\n\u001b[0;32m     18\u001b[0m all_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([f\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[59], line 88\u001b[0m, in \u001b[0;36mconvert_examples_to_features\u001b[1;34m(examples, label_list, max_seq_length, tokenizer, output_mode, cls_token_at_end, pad_on_left, cls_token, sep_token, pad_token, sequence_a_segment_id, sequence_b_segment_id, cls_token_segment_id, pad_token_segment_id, mask_padding_with_zero)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(segment_ids) \u001b[38;5;241m==\u001b[39m max_seq_length\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     label_id \u001b[38;5;241m=\u001b[39m label_map[example\u001b[38;5;241m.\u001b[39mlabel]\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     90\u001b[0m     label_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(example\u001b[38;5;241m.\u001b[39mlabel)\n",
      "\u001b[1;31mKeyError\u001b[0m: 45.0"
     ]
    }
   ],
   "source": [
    "data_dir= '../data/'\n",
    "model.to(device)\n",
    "train_dataset = load_and_cache_examples(tokenizer, dataset=\"train\")\n",
    "global_step, tr_loss = train(train_dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_24900\\3514849440.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc23f3f8f73f461d974b6eac2d2dfca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.889589905362776, 'Precision': 0.8895874036358471, 'Recall': 0.889589905362776, 'f1 Score': 0.889588602022941}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "result = evaluate(model, tokenizer, prefix=global_step)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 20:39:18] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 20:39:18] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 20:39:18] No GPU found.\n",
      "[codecarbon INFO @ 20:39:18] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 20:39:18] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 20:39:20] CPU Model on constant consumption mode: AMD Ryzen 5 5500U with Radeon Graphics\n",
      "[codecarbon INFO @ 20:39:20] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 20:39:20]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 20:39:20]   Python version: 3.11.5\n",
      "[codecarbon INFO @ 20:39:20]   CodeCarbon version: 2.8.1\n",
      "[codecarbon INFO @ 20:39:20]   Available RAM : 7.342 GB\n",
      "[codecarbon INFO @ 20:39:20]   CPU count: 12\n",
      "[codecarbon INFO @ 20:39:20]   CPU model: AMD Ryzen 5 5500U with Radeon Graphics\n",
      "[codecarbon INFO @ 20:39:20]   GPU count: None\n",
      "[codecarbon INFO @ 20:39:20]   GPU model: None\n",
      "[codecarbon INFO @ 20:39:21] Saving emissions data to file C:\\Users\\phili\\Sentiment Analysis with LLM-Based Mitigation Techniques for Cyberbullying Detection on Social Media Platforms\\Codes\\emissions.csv\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_24900\\3514849440.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4092573a82594f0082ce07afaf8b087f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 20:39:36] Energy consumed for RAM : 0.000011 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:39:36] Energy consumed for all CPUs : 0.000052 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:39:36] 0.000064 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:39:51] Energy consumed for RAM : 0.000023 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:39:51] Energy consumed for all CPUs : 0.000104 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:39:51] 0.000127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:40:06] Energy consumed for RAM : 0.000034 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:40:06] Energy consumed for all CPUs : 0.000156 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:40:06] 0.000191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:40:21] Energy consumed for RAM : 0.000046 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:40:21] Energy consumed for all CPUs : 0.000208 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:40:21] 0.000254 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:40:36] Energy consumed for RAM : 0.000057 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:40:36] Energy consumed for all CPUs : 0.000261 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:40:36] 0.000318 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:40:51] Energy consumed for RAM : 0.000069 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:40:51] Energy consumed for all CPUs : 0.000313 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:40:51] 0.000381 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:41:06] Energy consumed for RAM : 0.000080 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:41:06] Energy consumed for all CPUs : 0.000365 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:41:06] 0.000445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:41:21] Energy consumed for RAM : 0.000092 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:41:21] Energy consumed for all CPUs : 0.000417 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:41:21] 0.000509 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:41:21] 0.000869 g.CO2eq/s mean an estimation of 27.418567953315613 kg.CO2eq/year\n",
      "[codecarbon INFO @ 20:41:36] Energy consumed for RAM : 0.000103 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:41:36] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:41:36] 0.000572 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:41:51] Energy consumed for RAM : 0.000115 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:41:51] Energy consumed for all CPUs : 0.000521 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:41:51] 0.000636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:42:06] Energy consumed for RAM : 0.000126 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:42:06] Energy consumed for all CPUs : 0.000573 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:42:06] 0.000699 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:42:21] Energy consumed for RAM : 0.000138 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:42:21] Energy consumed for all CPUs : 0.000625 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:42:21] 0.000763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:42:36] Energy consumed for RAM : 0.000149 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:42:36] Energy consumed for all CPUs : 0.000677 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:42:36] 0.000826 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:42:51] Energy consumed for RAM : 0.000161 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:42:51] Energy consumed for all CPUs : 0.000729 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:42:51] 0.000890 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:43:06] Energy consumed for RAM : 0.000172 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:43:06] Energy consumed for all CPUs : 0.000782 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:43:06] 0.000954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:43:21] Energy consumed for RAM : 0.000184 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:43:21] Energy consumed for all CPUs : 0.000834 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:43:21] 0.001017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:43:21] 0.000870 g.CO2eq/s mean an estimation of 27.4209408824441 kg.CO2eq/year\n",
      "[codecarbon INFO @ 20:43:36] Energy consumed for RAM : 0.000195 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:43:36] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:43:36] 0.001081 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:43:51] Energy consumed for RAM : 0.000206 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:43:51] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:43:51] 0.001144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:44:06] Energy consumed for RAM : 0.000218 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:44:06] Energy consumed for all CPUs : 0.000990 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:44:06] 0.001208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:44:21] Energy consumed for RAM : 0.000229 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:44:21] Energy consumed for all CPUs : 0.001042 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:44:21] 0.001272 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:44:36] Energy consumed for RAM : 0.000241 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:44:36] Energy consumed for all CPUs : 0.001094 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:44:36] 0.001335 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:44:51] Energy consumed for RAM : 0.000252 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:44:51] Energy consumed for all CPUs : 0.001146 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:44:51] 0.001399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:44:51] Energy consumed for RAM : 0.000252 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 20:44:51] Energy consumed for all CPUs : 0.001147 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 20:44:51] 0.001399 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.889589905362776, 'Precision': 0.8895874036358471, 'Recall': 0.889589905362776, 'f1 Score': 0.889588602022941}\n",
      "Inference Time: 330.3777 seconds\n",
      "CPU Usage: 0.00%\n",
      "RAM Usage: -36.30 MB\n",
      "Energy Consumption: 0.000287 kWh\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "import time\n",
    "import psutil\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Start tracking energy consumption\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "# Measure CPU and RAM usage before evaluation\n",
    "process = psutil.Process()\n",
    "cpu_before = psutil.cpu_percent(interval=None)\n",
    "ram_before = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluation\n",
    "result = evaluate(model, tokenizer, prefix=global_step)\n",
    "\n",
    "# Measure inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Measure CPU and RAM usage after evaluation\n",
    "cpu_after = psutil.cpu_percent(interval=None)\n",
    "ram_after = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Calculate differences\n",
    "cpu_usage = cpu_after - cpu_before\n",
    "ram_usage = ram_after - ram_before\n",
    "\n",
    "# Stop energy tracking\n",
    "emissions = tracker.stop()\n",
    "\n",
    "# Print results\n",
    "print(result)\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"CPU Usage: {cpu_usage:.2f}%\")\n",
    "print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "print(f\"Energy Consumption: {emissions:.6f} kWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GIx38YdB_oHO"
   },
   "outputs": [],
   "source": [
    "def evaluatetest(model, tokenizer, prefix=\"\"):\r\n",
    "  results = {}\r\n",
    "  eval_dataset = load_and_cache_examples(tokenizer, dataset='test')\r\n",
    "  eval_batch_size = 8\r\n",
    "  eval_sampler = SequentialSampler(eval_dataset)\r\n",
    "  eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\r\n",
    "  eval_loss = 0.0\r\n",
    "  nb_eval_steps = 0\r\n",
    "  preds = None\r\n",
    "  out_label_ids = None\r\n",
    "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\r\n",
    "    model.eval()\r\n",
    "    batch = tuple(t.to(device) for t in batch)\r\n",
    "    with torch.no_grad():\r\n",
    "      inputs = {'input_ids':      batch[0],\r\n",
    "                'attention_mask': batch[1],\r\n",
    "                'token_type_ids': None,             # XLM and RoBERTa don't use segment_ids\r\n",
    "                'labels':         batch[3]}\r\n",
    "      outputs = model(**inputs)\r\n",
    "      tmp_eval_loss, logits = outputs[:2]\r\n",
    "      eval_loss += tmp_eval_loss.mean().item()\r\n",
    "    \r\n",
    "    nb_eval_steps += 1\r\n",
    "    if preds is None:\r\n",
    "        preds = logits.detach().cpu().numpy()\r\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\r\n",
    "    else:\r\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\r\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\r\n",
    "  eval_loss = eval_loss / nb_eval_steps\r\n",
    "  if output_mode == \"classification\":\r\n",
    "      preds = np.argmax(preds, axis=1)\r\n",
    "  elif output_mode == \"regression\":\r\n",
    "      preds = np.squeeze(preds)\r\n",
    "  result = compute_metrics(\"tweet\", preds, out_label_ids)\r\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_24900\\3514849440.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c597c3b3bde4918a46e797626cb40e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.889589905362776, 'Precision': 0.8895874036358471, 'Recall': 0.889589905362776, 'f1 Score': 0.889588602022941}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "result = evaluate(model, tokenizer, prefix=global_step)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:02:10] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:02:10] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:02:10] No GPU found.\n",
      "[codecarbon INFO @ 21:02:10] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:02:10] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 21:02:12] CPU Model on constant consumption mode: AMD Ryzen 5 5500U with Radeon Graphics\n",
      "[codecarbon INFO @ 21:02:12] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:02:12]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 21:02:12]   Python version: 3.11.5\n",
      "[codecarbon INFO @ 21:02:12]   CodeCarbon version: 2.8.1\n",
      "[codecarbon INFO @ 21:02:12]   Available RAM : 7.342 GB\n",
      "[codecarbon INFO @ 21:02:12]   CPU count: 12\n",
      "[codecarbon INFO @ 21:02:12]   CPU model: AMD Ryzen 5 5500U with Radeon Graphics\n",
      "[codecarbon INFO @ 21:02:12]   GPU count: None\n",
      "[codecarbon INFO @ 21:02:12]   GPU model: None\n",
      "[codecarbon INFO @ 21:02:13] Saving emissions data to file C:\\Users\\phili\\Sentiment Analysis with LLM-Based Mitigation Techniques for Cyberbullying Detection on Social Media Platforms\\Codes\\emissions.csv\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_24900\\3514849440.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a97be82643419b806d5be892772b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:02:28] Energy consumed for RAM : 0.000011 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:02:28] Energy consumed for all CPUs : 0.000052 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:02:28] 0.000064 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:02:43] Energy consumed for RAM : 0.000023 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:02:43] Energy consumed for all CPUs : 0.000104 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:02:43] 0.000127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:02:58] Energy consumed for RAM : 0.000034 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:02:58] Energy consumed for all CPUs : 0.000156 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:02:58] 0.000191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:13] Energy consumed for RAM : 0.000046 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:03:13] Energy consumed for all CPUs : 0.000208 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:03:13] 0.000254 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:28] Energy consumed for RAM : 0.000057 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:03:28] Energy consumed for all CPUs : 0.000261 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:03:28] 0.000318 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:43] Energy consumed for RAM : 0.000069 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:03:43] Energy consumed for all CPUs : 0.000313 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:03:43] 0.000382 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:03:58] Energy consumed for RAM : 0.000080 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:03:58] Energy consumed for all CPUs : 0.000365 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:03:58] 0.000445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:13] Energy consumed for RAM : 0.000092 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:04:13] Energy consumed for all CPUs : 0.000417 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:04:13] 0.000509 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:13] 0.000869 g.CO2eq/s mean an estimation of 27.41566884343416 kg.CO2eq/year\n",
      "[codecarbon INFO @ 21:04:28] Energy consumed for RAM : 0.000103 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:04:28] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:04:28] 0.000572 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:43] Energy consumed for RAM : 0.000115 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:04:43] Energy consumed for all CPUs : 0.000521 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:04:43] 0.000636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:04:58] Energy consumed for RAM : 0.000126 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:04:58] Energy consumed for all CPUs : 0.000573 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:04:58] 0.000699 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:05:13] Energy consumed for RAM : 0.000138 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:05:13] Energy consumed for all CPUs : 0.000625 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:05:13] 0.000763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:05:28] Energy consumed for RAM : 0.000149 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:05:28] Energy consumed for all CPUs : 0.000677 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:05:28] 0.000826 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:05:44] Energy consumed for RAM : 0.000161 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:05:44] Energy consumed for all CPUs : 0.000730 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:05:44] 0.000890 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:05:59] Energy consumed for RAM : 0.000172 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:05:59] Energy consumed for all CPUs : 0.000782 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:05:59] 0.000954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:06:14] Energy consumed for RAM : 0.000184 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:06:14] Energy consumed for all CPUs : 0.000834 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:06:14] 0.001017 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:06:14] 0.000869 g.CO2eq/s mean an estimation of 27.416837339015576 kg.CO2eq/year\n",
      "[codecarbon INFO @ 21:06:29] Energy consumed for RAM : 0.000195 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:06:29] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:06:29] 0.001081 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:06:44] Energy consumed for RAM : 0.000206 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:06:44] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:06:44] 0.001144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:06:59] Energy consumed for RAM : 0.000218 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:06:59] Energy consumed for all CPUs : 0.000990 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:06:59] 0.001208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:07:14] Energy consumed for RAM : 0.000229 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:07:14] Energy consumed for all CPUs : 0.001042 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:07:14] 0.001272 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:07:29] Energy consumed for RAM : 0.000241 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:07:29] Energy consumed for all CPUs : 0.001094 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:07:29] 0.001335 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:07:44] Energy consumed for RAM : 0.000252 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:07:44] Energy consumed for all CPUs : 0.001146 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:07:44] 0.001399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:07:59] Energy consumed for RAM : 0.000264 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:07:59] Energy consumed for all CPUs : 0.001198 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:07:59] 0.001462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:08:14] Energy consumed for RAM : 0.000275 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:08:14] Energy consumed for all CPUs : 0.001251 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:08:14] 0.001526 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:08:14] 0.000869 g.CO2eq/s mean an estimation of 27.417167487287077 kg.CO2eq/year\n",
      "[codecarbon INFO @ 21:08:29] Energy consumed for RAM : 0.000287 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:08:29] Energy consumed for all CPUs : 0.001303 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:08:29] 0.001589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:08:34] Energy consumed for RAM : 0.000291 kWh. RAM Power : 2.7531466484069824 W\n",
      "[codecarbon INFO @ 21:08:34] Energy consumed for all CPUs : 0.001321 kWh. Total CPU Power : 12.5 W\n",
      "[codecarbon INFO @ 21:08:34] 0.001612 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.889589905362776, 'Precision': 0.8895874036358471, 'Recall': 0.889589905362776, 'f1 Score': 0.889588602022941}\n",
      "Inference Time: 380.4975 seconds\n",
      "CPU Usage: 33.60%\n",
      "RAM Usage: -356.00 MB\n",
      "Energy Consumption: 0.000331 kWh\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "#calculate Inference time, RAM usage, CPU usage, and Energy consumption during Evaluation\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Start tracking energy consumption\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "# Measure CPU and RAM usage before evaluation\n",
    "process = psutil.Process()\n",
    "cpu_before = psutil.cpu_percent(interval=None)\n",
    "ram_before = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluation\n",
    "result = evaluate(model, tokenizer, prefix=global_step)\n",
    "\n",
    "# Measure inference time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Measure CPU and RAM usage after evaluation\n",
    "cpu_after = psutil.cpu_percent(interval=None)\n",
    "ram_after = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Calculate differences\n",
    "cpu_usage = cpu_after - cpu_before\n",
    "ram_usage = ram_after - ram_before\n",
    "\n",
    "# Stop energy tracking\n",
    "emissions = tracker.stop()\n",
    "\n",
    "# Print results\n",
    "print(result)\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"CPU Usage: {cpu_usage:.2f}%\")\n",
    "print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "print(f\"Energy Consumption: {emissions:.6f} kWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RwuIGFk4_ky"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6Tgzbr04_nQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "8. RoBERTa.ipynb",
   "provenance": [
    {
     "file_id": "1dLIJOZ9Ar0G88j2Sreqg5nokTNm0UAyM",
     "timestamp": 1608768028418
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "142219a840f3408d9b951aa884fa4362": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14fd1204a16d4c3692e57e2852041c2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36db65bfa8214685b95ea2ea46b81bda",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_18ac3a6a121c4a18910759f5c69e22f1",
      "value": 3
     }
    },
    "18ac3a6a121c4a18910759f5c69e22f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1acce1360fef474aa86b9abd11d6faab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cd501aa81f345d185b19a3f741e50d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Evaluating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_368dd39a241b4903ae2a084be4697c7a",
      "max": 685,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca515671c27c4905834982fd360c6649",
      "value": 685
     }
    },
    "244e5d1e610e4e7180f83fa6e3ab8ab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "305cb5e80a6943e6a714908a088c0398": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "368dd39a241b4903ae2a084be4697c7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36db65bfa8214685b95ea2ea46b81bda": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a2ae730cb3d470ab40b96c8e4e67058": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f1bbd86cedd44d79d5b9bbb1f502bd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "43dd7167468747d9aac84ae9a003e42f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_142219a840f3408d9b951aa884fa4362",
      "max": 2405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f1bbd86cedd44d79d5b9bbb1f502bd4",
      "value": 2405
     }
    },
    "4f78a0941ba24435b1ad3f9de377fcd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "500f8f5204db452f9ec05c1e392eeb00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9f7fb247b0a41a0b25eef76e8337cd8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d35c7207643f49dfa3b1f4fb6a81de81",
      "value": " 685/685 [00:40&lt;00:00, 16.86it/s]"
     }
    },
    "6117124ef56747cb9660a3b0a1320814": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e803ea8467d848888de63596f4ee1920",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_244e5d1e610e4e7180f83fa6e3ab8ab7",
      "value": " 3/3 [12:23&lt;00:00, 248.00s/it]"
     }
    },
    "638ece11c7014635a228491add1be51d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ec0c4d438e34833979e9caa837ba5a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738d1f6643bb4d0784fc279254b24134": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82b8beb2e5284052846d9e1a554b65e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "853d9a7b580a474aafa0941123ff863b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "866d76086148468f8409464495f5b65e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_305cb5e80a6943e6a714908a088c0398",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aff54d05e66d4eddb83fec133c002263",
      "value": " 2405/2405 [09:09&lt;00:00,  4.38it/s]"
     }
    },
    "91a9349ed14b49cea0bd1d2762dfa7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_638ece11c7014635a228491add1be51d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b75ec03a81894ce6a13e232ad16222ab",
      "value": " 2405/2405 [05:54&lt;00:00,  6.79it/s]"
     }
    },
    "a125e2c861bf40f88ec5448bb0ac14a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aa163f956a7a48669f70ebbc50912fa9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa165c4c8d4a4bdba30d669f59906d15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14fd1204a16d4c3692e57e2852041c2b",
       "IPY_MODEL_6117124ef56747cb9660a3b0a1320814"
      ],
      "layout": "IPY_MODEL_6ec0c4d438e34833979e9caa837ba5a7"
     }
    },
    "aff54d05e66d4eddb83fec133c002263": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b75ec03a81894ce6a13e232ad16222ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c980297feeca4330a7ec8cd7b3611d63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43dd7167468747d9aac84ae9a003e42f",
       "IPY_MODEL_91a9349ed14b49cea0bd1d2762dfa7cf"
      ],
      "layout": "IPY_MODEL_738d1f6643bb4d0784fc279254b24134"
     }
    },
    "ca3c971fe77d454eaf3179146487dc44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd664e45c6b24bc791ab4c063dd97b01",
       "IPY_MODEL_866d76086148468f8409464495f5b65e"
      ],
      "layout": "IPY_MODEL_82b8beb2e5284052846d9e1a554b65e2"
     }
    },
    "ca515671c27c4905834982fd360c6649": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cd664e45c6b24bc791ab4c063dd97b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa163f956a7a48669f70ebbc50912fa9",
      "max": 2405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a125e2c861bf40f88ec5448bb0ac14a4",
      "value": 2405
     }
    },
    "d35c7207643f49dfa3b1f4fb6a81de81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc452a3c7a0842309e44f5a32c1b82bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8a92849a76642a7ab2b186ae0df4ae7",
       "IPY_MODEL_eaa7ef95bed7416aa97237bcff09b7af"
      ],
      "layout": "IPY_MODEL_e8412f4d3b8e46ab81d4f301157f00be"
     }
    },
    "dfcc3e8bb29142aaaf0cf2a9d4ecce92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1cd501aa81f345d185b19a3f741e50d4",
       "IPY_MODEL_500f8f5204db452f9ec05c1e392eeb00"
      ],
      "layout": "IPY_MODEL_1acce1360fef474aa86b9abd11d6faab"
     }
    },
    "e803ea8467d848888de63596f4ee1920": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8412f4d3b8e46ab81d4f301157f00be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8a92849a76642a7ab2b186ae0df4ae7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f78a0941ba24435b1ad3f9de377fcd7",
      "max": 2405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_853d9a7b580a474aafa0941123ff863b",
      "value": 2405
     }
    },
    "eaa7ef95bed7416aa97237bcff09b7af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ead4e7718c7a497bb0bc54fe8e19ae35",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3a2ae730cb3d470ab40b96c8e4e67058",
      "value": " 2405/2405 [12:23&lt;00:00,  3.23it/s]"
     }
    },
    "ead4e7718c7a497bb0bc54fe8e19ae35": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9f7fb247b0a41a0b25eef76e8337cd8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
